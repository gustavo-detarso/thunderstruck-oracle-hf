# Or√°culo MPS ‚Äî Fine-tuning, RAG e Busca Sem√¢ntica em LLM

Este reposit√≥rio cont√©m uma solu√ß√£o completa para:
- **Gera√ß√£o autom√°tica de pares Pergunta/Resposta (QA)**
- **Fine-tuning eficiente de modelos LLM (LoRA)**
- **Busca sem√¢ntica usando FAISS + Sentence Transformers**
- **RAG (Retrieval Augmented Generation) com interface de avalia√ß√£o do usu√°rio**

O fluxo cobre desde o processamento dos dados at√© o deploy do modelo treinado e a busca inteligente por contexto.

---

## üöÄ **Fluxo resumido do projeto**

1. **Gera√ß√£o dos dados QA**  
   - Convers√£o de arquivos .txt (editais, portarias, tabelas) em arquivos `.jsonl` de QA autom√°ticos.
2. **Indexa√ß√£o sem√¢ntica**  
   - Cria√ß√£o de um √≠ndice FAISS para busca r√°pida de contexto relevante.
3. **Fine-tuning LoRA em LLM**  
   - Treinamento do modelo com LoRA (efficient fine-tuning).
4. **Mesclagem e deploy**  
   - Merge dos adapters LoRA, push para o HuggingFace Hub, c√°lculo do custo de GPU.
5. **Busca RAG**  
   - Busca e gera√ß√£o de respostas usando o modelo treinado + contexto recuperado pelo √≠ndice.

---

## üóÉÔ∏è **Arquivos e diret√≥rios principais**

- `preprocessing/gen_qa_chatgpt_from_txt.py`  
  Extrai QAs autom√°ticos de arquivos `.txt`, detecta tabelas, tags e cria arquivos `.jsonl` para cada documento.

- `faiss_index.py`  
  Indexa todos os QAs em um vetor sem√¢ntico (usando [Sentence Transformers](https://www.sbert.net/)), salva um √≠ndice FAISS (`faiss_chunks.index`) e o mapeamento original (`faiss_mapping.pkl`).

- `autogen-model-hf.py`  
  Script de treinamento LoRA.  
  - Usa [HuggingFace Transformers](https://huggingface.co/docs/transformers), [PEFT](https://github.com/huggingface/peft) e PyTorch.
  - Ao final, salva tanto o *adapter* LoRA (`lora_llama_finetuned/`) quanto o modelo mesclado (`merged_model/`).
  - Calcula custo do treinamento baseado no tempo e valor do droplet, consultando a cota√ß√£o do d√≥lar turismo via [AwesomeAPI](https://docs.awesomeapi.com.br/api-de-moedas).

- `rag_qa_finetuned.py`  
  Interface RAG via linha de comando:  
  - Usa embeddings para buscar o contexto mais relevante em FAISS,
  - Monta o prompt e gera resposta com o modelo fine-tuned,
  - Loga e classifica QAs para refor√ßo futuro do dataset.

- `data_qa/`  
  Pasta com todos os arquivos `.jsonl` gerados (QAs, logs do usu√°rio, dados validados e inv√°lidos).

- `lora_llama_finetuned/`  
  Adapter LoRA salvo ap√≥s o fine-tuning.

- `merged_model/`  
  Modelo LLM final com LoRA mesclado, pronto para deploy ou infer√™ncia stand-alone.

- `faiss_chunks.index`, `faiss_mapping.pkl`  
  √çndice FAISS e mapeamento dos chunks para busca sem√¢ntica.

---

## üì¶ **Tecnologias e bibliotecas utilizadas**

- **[HuggingFace Transformers](https://huggingface.co/docs/transformers)**  
  Framework l√≠der para modelos de linguagem (LLMs), NLP, gera√ß√£o e fine-tuning.

- **[PEFT (Parameter-Efficient Fine-Tuning)](https://github.com/huggingface/peft)**  
  Permite treinar grandes modelos com poucos par√¢metros extras, reduzindo custo e mem√≥ria.  
  No projeto, usado para LoRA (Low-Rank Adaptation) ‚Äî fine-tuning eficiente de LLMs.

- **[Sentence Transformers](https://www.sbert.net/)**  
  Modelos para embeddings de senten√ßas e busca sem√¢ntica r√°pida.

- **[FAISS](https://github.com/facebookresearch/faiss)**  
  Biblioteca da Meta/Facebook para busca vetorial e indexa√ß√£o ultra-r√°pida (usada para o retrieval de contexto).

- **[PyTorch](https://pytorch.org/)**  
  Framework de deep learning para treinamento e infer√™ncia dos modelos.

- **[scikit-learn](https://scikit-learn.org/)**  
  Utilizado para split de datasets e manipula√ß√£o b√°sica.

- **[tqdm](https://tqdm.github.io/)**  
  Barra de progresso para loops de treinamento e indexa√ß√£o.

- **[TensorBoard](https://www.tensorflow.org/tensorboard)**  
  Para monitoramento visual do progresso e m√©tricas durante o treinamento.

- **[requests](https://requests.readthedocs.io/)**  
  Usado para buscar a cota√ß√£o do d√≥lar turismo em tempo real (API AwesomeAPI).

- **[HuggingFace Hub](https://huggingface.co/docs/hub)**  
  Armazenamento, versionamento e deploy de modelos, inclusive privados.

---

## üìù **Como rodar o projeto**

1. **Prepare seu ambiente**  
   - Instale as depend√™ncias (idealmente usando um ambiente virtual):
     ```bash
     pip install -r requirements.txt
     ```

2. **Gere QAs a partir de arquivos .txt**
   ```bash
   python preprocessing/gen_qa_chatgpt_from_txt.py
   ```

3. **Crie/atualize o √≠ndice sem√¢ntico FAISS**
   ```bash
   python faiss_index.py
   ```

4. **Treine o modelo com LoRA**
   ```bash
   python autogen-model-hf.py
   ```

5. **Use a busca RAG para responder perguntas**
   ```bash
   python rag_qa_finetuned.py
   ```

---

## üö¶ **Dicas de uso/ajustes**

- Ajuste o caminho dos diret√≥rios, nomes de modelos e par√¢metros conforme sua infra.
- O fine-tuning pode ser feito em GPU local, cloud ou qualquer infra com CUDA e RAM suficiente.
- Todos os arquivos `.jsonl` em `data_qa/` podem ser incrementados manualmente ou pelo feedback dos usu√°rios (human-in-the-loop).

---

## ü§ñ **Sobre o fluxo LoRA**

- **LoRA adapter** (`lora_llama_finetuned/`): permite reusar e continuar o fine-tuning incrementalmente, ou aplicar sobre outros modelos base.
- **Modelo mesclado** (`merged_model/`): para deploy stand-alone (inference sem PEFT).

---

## üõ°Ô∏è **.gitignore sugerido**

O projeto inclui um `.gitignore` para evitar versionar:
- Ambientes virtuais,
- Dados sens√≠veis,
- Checkpoints/modelos grandes,
- Indexes tempor√°rios e logs.

---

## üè∑Ô∏è **Licen√ßa**

Este projeto segue a licen√ßa [MIT](LICENSE) (ou ajuste conforme sua necessidade).

---

# Qualquer d√∫vida ou contribui√ß√£o, sinta-se √† vontade para abrir uma issue ou PR!


